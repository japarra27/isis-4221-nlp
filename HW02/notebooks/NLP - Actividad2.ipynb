{"cells":[{"cell_type":"markdown","source":["# NLP - HW2"],"metadata":{}},{"cell_type":"markdown","source":["Con el objetivo de aplicar los conocimientos presentados en clase y a su vez generar los modelos solicitados, en este punto se hará uso de la herramienta Databricks y Pyspark con el objetivo de leer los archivos, generar la construcción del dataset y hacer el entrenamiento respectivo de los modelos.\n\nPara la elaboración de la actividad se sigue una serie de pasos:\n\n* Se crea un cluster de spark 3.1 en Databricks, con mínimo 1 driver, 1 nodo y autoescalable a máximo 20.\n* Cada nodo es una instancia m4.large de AWS con 2 core y 8gb ram, el driver tiene la misma configuración.\n* Los archivos se almacenan en un bucket de s3, se descomprime y se hace la lectura para la construcción del dataset.\n* Se hace uso de spark para paralelizar los trabajos y hacerlos más eficiente a la hora de entrenar los modelos."],"metadata":{}},{"cell_type":"markdown","source":["## Punto No. 2: Naive Bayes (NB) & Logistic Regression (LR)\n\n### For the 20N dataset compare two classifiers NB and LR to identify the 20 different newsgroups."],"metadata":{}},{"cell_type":"markdown","source":["##### Create your own processing pipeline for the task and justify it."],"metadata":{}},{"cell_type":"markdown","source":["Para esta primera parte se hará la extracción de los archivos de 20news para poder leer e iterar sobre ellos con el objetivo de construir el dataset.\n\nPara hacer el proceso de extracción de forma eficiente, se utiliza awscli en la maquina y se hace la extracción de archivos en el bucket, una vez extraida la información se valida que en el bucket se encuentren todos los archivos, que en total son 18.828 y 20 carpetas con cada categoría."],"metadata":{}},{"cell_type":"code","source":["# importar librerias\n\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import *\n\n# Librerias para procesar datos\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import CountVectorizer, StringIndexer\n\n# librerias sparknlp\nimport sparknlp\nfrom sparknlp.base import *\nfrom sparknlp.common import *\nfrom sparknlp.annotator import *"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Definición de esquema para la creación del dataset\n\nfileSchema = StructType(\n    [\n        StructField(\"filePath\", StringType(), True),\n        StructField(\"fileText\", StringType(), True),\n    ]\n)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# leer todos los archivos\n\nsdf = sc.wholeTextFiles(\"/mnt/databricks-mine/HW02/20news-18828/*\").toDF(\n    schema=fileSchema\n)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Proceso de limpieza y organización de texto\n\nsdf = sdf.withColumn(\"fileName\", F.split(sdf[\"filePath\"], \"/\").getItem(5)).withColumn(\n    \"processedText\", F.regexp_replace(sdf.fileText, '[_():;,!?\\\\->@<~^_`}\"{/]', \" \")).withColumn(\"fileNumber\", F.split(sdf[\"filePath\"], \"/\").getItem(6))\n\nsdf = sdf.withColumn(\"processedText\", F.regexp_replace(sdf.processedText, \"[.$\\[\\]|*\\\\']\", \"\"))\nsdf = sdf.withColumn(\"processedText\", F.regexp_replace(sdf.processedText, \"[0-9]\", \" \"))\nsdf = sdf.withColumn(\"processedText\", F.regexp_replace(sdf.processedText, \"\\n\", \"\"))\nsdf = sdf.withColumn('processedText', F.regexp_replace(sdf.processedText, ' +', ' '))\nsdf = sdf.withColumn('processedText', F.lower(sdf.processedText))\n\n# Seleccionar las columnas a procesar para los modelos\n\nsdf = sdf.select('processedText', 'fileName', 'fileNumber')"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Definición del pipeline para el procesamiento de texto\n\ndocument = DocumentAssembler()\\\n    .setInputCol(\"processedText\")\\\n    .setOutputCol(\"document\")\n\ntoken = Tokenizer()\\\n    .setInputCols(['document'])\\\n    .setOutputCol('token')\n\nnormalice = Normalizer()\\\n    .setInputCols(['token'])\\\n    .setOutputCol('token_norm')\\\n    .setLowercase(True)\\\n    .setCleanupPatterns([\"[^\\w\\s]\"])\n\nstop_words = StopWordsCleaner.pretrained('stopwords_en', 'en')\\\n    .setInputCols([\"token_norm\"]) \\\n    .setOutputCol(\"cleanTokens\") \\\n    .setCaseSensitive(False)\n\ntokenassembler = TokenAssembler()\\\n    .setInputCols([\"document\", \"cleanTokens\"]) \\\n    .setOutputCol(\"clean_text\")\n\nprediction_pipeline = Pipeline(\n    stages = [\n        document,\n        token,\n        normalice,\n        stop_words,\n        tokenassembler\n    ]\n)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">stopwords_en download started this may take some time.\nApproximate size to download 2.9 KB\n\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[OK!]\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# fit y transform del pipeline con la información de 20news\nresult = prediction_pipeline.fit(sdf).transform(sdf)\n\n\n# Selección de columnas para la ejecución del modelo\nresult = (\n    result.withColumn(\"tokens\", F.col(\"cleanTokens.result\"))\n    .withColumn(\"text\", F.col(\"clean_text.result\"))\n    .select(\"fileName\", \"fileNumber\", \"tokens\", \"text\")\n)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["# Guardar la data procesada en formato parquet para aumentar eficiencia al momento de leer y entrenar modelos.\nresult.write.partitionBy([\"fileName\"]).format('parquet').mode(\"append\").save('/mnt/databricks-mine/HW02/news20processed')"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Eliminación de variables no necesarias\ndel sdf, result"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Importar dataset procesado para la ejecución de los modelos\nsdf = spark.read.format('parquet').load('/mnt/databricks-mine/HW02/news20processed')"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["indexer = StringIndexer(inputCol=\"fileName\", outputCol=\"labelIndex\")\nfeature_data = indexer.fit(sdf).transform(sdf)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["feature_data.select(\"fileName\",\"labelIndex\").groupBy('fileName', 'labelIndex').count().orderBy('labelIndex').show(25)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+----------+-----+\n            fileName|labelIndex|count|\n+--------------------+----------+-----+\n    rec.sport.hockey|       0.0|  999|\nsoc.religion.chri...|       1.0|  997|\n     rec.motorcycles|       2.0|  994|\n  rec.sport.baseball|       3.0|  994|\n           sci.crypt|       4.0|  991|\n           rec.autos|       5.0|  990|\n             sci.med|       6.0|  990|\n           sci.space|       7.0|  987|\ncomp.os.ms-window...|       8.0|  985|\ncomp.sys.ibm.pc.h...|       9.0|  982|\n     sci.electronics|      10.0|  981|\n      comp.windows.x|      11.0|  980|\n       comp.graphics|      12.0|  973|\n        misc.forsale|      13.0|  972|\ncomp.sys.mac.hard...|      14.0|  961|\ntalk.politics.mid...|      15.0|  940|\n  talk.politics.guns|      16.0|  910|\n         alt.atheism|      17.0|  799|\n  talk.politics.misc|      18.0|  775|\n  talk.religion.misc|      19.0|  628|\n+--------------------+----------+-----+\n\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["##### Divide the dataset into training (60%), development (10%) and test (30%)"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["##### Train NB and LR using the following representations:"],"metadata":{}},{"cell_type":"markdown","source":["###### BOW"],"metadata":{}},{"cell_type":"markdown","source":["###### Boolean BOW"],"metadata":{}},{"cell_type":"markdown","source":["###### Personalized representation. You as a designer must define the select set of characteristics. Explain your feature selection strategy in detail."],"metadata":{}},{"cell_type":"markdown","source":["#### Compare the results of NB and LR using 10-fold cross validation:\n\n##### Use for cross validation: training+development sets."],"metadata":{}},{"cell_type":"markdown","source":["##### Do a search for LR hyperparameters"],"metadata":{}},{"cell_type":"markdown","source":["##### Report the average results, precision, and recall by class."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["#### Evaluate models using the test set:\n\n##### Precision, recall, F1, accuracy with the micro and macro averaging strategies."],"metadata":{}},{"cell_type":"markdown","source":["#### Compare the results in terms of:\n\n##### NB vs LR"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["##### Features"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["##### Dataset and clasess distribution."],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/databricks-mine/HW02/"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["file_rdd = spark.read.text(\"/mnt/databricks-mine/HW02/20news-18828.tar.gz\", wholetext=True).rdd"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["file_rdd[0]"],"metadata":{},"outputs":[],"execution_count":35}],"metadata":{"name":"HW02 - NLP - Act2","notebookId":158},"nbformat":4,"nbformat_minor":0}